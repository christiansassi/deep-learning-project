{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaA4ccjTR4uV",
        "outputId": "6e47eab1-09be-48ff-c820-b2c46160cd44"
      },
      "outputs": [],
      "source": [
        "# Init workspace\n",
        "!rm -r dataset\n",
        "!mkdir dataset\n",
        "\n",
        "# Download dataset and extract it\n",
        "!gdown 111HiEoEvZDdg1Y2EefI6n5dA_p4sMV4V\n",
        "!mv imagenet-a.tar ./dataset\n",
        "!tar -xf ./dataset/imagenet-a.tar\n",
        "!mv imagenet-a ./dataset\n",
        "\n",
        "# Cleanup\n",
        "!rm ./dataset/imagenet-a.tar\n",
        "\n",
        "# (optional) Upgrading pytorch for the latest augmentation functions\n",
        "#!pip install --upgrade torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUzDPvD_e8Fq",
        "outputId": "7bbbd45d-6411-4dc5-dfed-c413b555cfff"
      },
      "outputs": [],
      "source": [
        "# Init workspace\n",
        "!rm -r dataset\n",
        "!mkdir dataset\n",
        "\n",
        "# Download dataset and extract it\n",
        "!gdown 1WKQGHjHUkIwZT0P2TpU9h-lY-6CnrsDd\n",
        "!mv imagenetv2-matched-frequency.tar.gz ./dataset\n",
        "!tar -xf ./dataset/imagenetv2-matched-frequency.tar.gz\n",
        "!mv imagenetv2-matched-frequency-format-val ./dataset\n",
        "\n",
        "# Cleanup\n",
        "!rm ./dataset/imagenetv2-matched-frequency.tar.gz\n",
        "\n",
        "# (optional) Upgrading pytorch for the latest augmentation functions\n",
        "#!pip install --upgrade torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "ECdvhkFCpxci"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import basename, isfile, join\n",
        "from pathlib import Path\n",
        "import requests\n",
        "from contextlib import nullcontext\n",
        "from copy import deepcopy\n",
        "from typing import Union\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import v2\n",
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "xMgaZ3ON9hNd"
      },
      "outputs": [],
      "source": [
        "# Use cuda if available\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SIZE = (224, 224)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "pOveYdeygYdb"
      },
      "outputs": [],
      "source": [
        "def load_imagenet_v2_labels() -> list[int]:\n",
        "\n",
        "    imagenet_v2 = \"./dataset/imagenetv2-matched-frequency-format-val\"\n",
        "\n",
        "    labels = [int(f) for f in listdir(imagenet_v2) if not isfile(join(imagenet_v2, f))]\n",
        "    labels.sort()\n",
        "\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "scjsACtS7Mzz"
      },
      "outputs": [],
      "source": [
        "def load_model_labels() -> list[str]:\n",
        "\n",
        "    url = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",
        "    path = Path(basename(url))\n",
        "\n",
        "    # Check if labels file already exists\n",
        "    if not path.exists():\n",
        "        response = requests.get(url)\n",
        "        path.write_text(response.text)\n",
        "\n",
        "    # Load labels\n",
        "    with open(path, \"r\") as f:\n",
        "        labels = json.load(f)\n",
        "\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "mZ4Mo02l8cqQ"
      },
      "outputs": [],
      "source": [
        "def load_model(model_name: str = \"google/vit-base-patch16-224\") -> ViTForImageClassification:\n",
        "\n",
        "    # Load the pre-trained model\n",
        "    return ViTForImageClassification.from_pretrained(model_name).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "X5w5Qab7nSxW"
      },
      "outputs": [],
      "source": [
        "class ImageNetV2(torch.utils.data.Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = load_imagenet_v2_labels() * 10\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        label = idx // 10\n",
        "\n",
        "        img_folder = os.path.join(self.img_dir, str(label))\n",
        "        img_path = [join(img_folder, f) for f in listdir(img_folder) if isfile(join(img_folder, f))][idx % (label if label != 0 else 1) - 1]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "ut7hOuKqfnct"
      },
      "outputs": [],
      "source": [
        "def load_dataset(resize: bool = True) -> torch.utils.data.dataloader.DataLoader:\n",
        "\n",
        "    imagenet_v2 = \"./dataset/imagenetv2-matched-frequency-format-val\"\n",
        "\n",
        "    # Prepare data transformations for the train loader\n",
        "    transforms = [] if not resize else [T.Resize(SIZE)]\n",
        "    transforms.append(T.ToTensor())\n",
        "    transform = T.Compose(transforms)\n",
        "\n",
        "    # Load data\n",
        "    imagenet_v2_dataset = ImageNetV2(annotations_file=[], img_dir=imagenet_v2, transform=transform)\n",
        "    return torch.utils.data.DataLoader(imagenet_v2_dataset, 1, shuffle=True, num_workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "Bk0WB8NHXXlQ"
      },
      "outputs": [],
      "source": [
        "def classify(model: ViTForImageClassification, img: torch.Tensor, no_grad: bool = True) -> dict:\n",
        "\n",
        "    # Use GPU if available\n",
        "    img = img.to(DEVICE)\n",
        "\n",
        "    # Perform inference\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad() if no_grad else nullcontext():\n",
        "        outputs = model(img)\n",
        "\n",
        "    # Extract probabilities from model's output logits\n",
        "    results = torch.nn.functional.softmax(outputs.logits, dim=-1).squeeze()\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "iLeoCaOmWG2u"
      },
      "outputs": [],
      "source": [
        "def elaborate_results(results: torch.Tensor) -> Union[dict, list]:\n",
        "\n",
        "    # Load model's labels\n",
        "    model_labels = load_model_labels()\n",
        "\n",
        "    if len(results.shape) == 1:\n",
        "        results = [results]\n",
        "\n",
        "    # Process results\n",
        "    final_results = []\n",
        "\n",
        "    for result in results:\n",
        "\n",
        "        item_results = {\n",
        "            \"predicted\": {},\n",
        "            \"results\": {}\n",
        "        }\n",
        "\n",
        "        predicted = None\n",
        "\n",
        "        for index, probability in enumerate(result):\n",
        "\n",
        "            item_results[\"results\"][index] = {\n",
        "                \"index\": index,\n",
        "                \"label\": model_labels[index],\n",
        "                \"probability\": probability.item()\n",
        "            }\n",
        "\n",
        "            if predicted is None or predicted[\"probability\"] < probability.item():\n",
        "                predicted = item_results[\"results\"][index]\n",
        "\n",
        "        item_results[\"predicted\"] = predicted\n",
        "\n",
        "        final_results.append(item_results)\n",
        "\n",
        "    return final_results if len(final_results) > 1 else final_results[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "jrXGkChUYXHS"
      },
      "outputs": [],
      "source": [
        "# # Load model (only once)\n",
        "# model = load_model()\n",
        "\n",
        "# # Load data (only once)\n",
        "# data_loader = load_dataset()\n",
        "\n",
        "# # Evaluate the model (Accuracy: 18.37 %)\n",
        "# accuracy = 0\n",
        "\n",
        "# for index, img in enumerate(data_loader):\n",
        "\n",
        "#     # Get model prediction\n",
        "#     results = classify(model=model, img=img[0])\n",
        "#     results = elaborate_results(results=results)\n",
        "#     predicted, results = results[\"predicted\"], results[\"results\"]\n",
        "\n",
        "#     if img[1].item() == predicted[\"index\"]:\n",
        "#         accuracy = accuracy + 1\n",
        "\n",
        "#     print(f\"Image {index+1} / {len(data_loader)} | Accuracy: {round((accuracy / (index + 1)) * 100, 2)}% ({accuracy} / {index + 1})\")\n",
        "\n",
        "# accuracy = accuracy / len(data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "IzaEQC-_dzji",
        "outputId": "9864cdc7-6f63-48bb-8bea-333b2449cf2d"
      },
      "outputs": [],
      "source": [
        "# Supponendo che load_dataset, classify_image, merged_labels e il modello siano definiti altrove\n",
        "accuracy_before = 0\n",
        "accuracy_after = 0\n",
        "\n",
        "# Load model (only once)\n",
        "model = load_model()\n",
        "original_model = deepcopy(model)\n",
        "\n",
        "data_loader = load_dataset(resize=False)\n",
        "\n",
        "# transformation = T.Compose([\n",
        "#         T.Resize((500, 500)),\n",
        "#         T.CenterCrop((384, 384)) ])\n",
        "\n",
        "transforms = v2.Compose([\n",
        "    v2.RandomResizedCrop(size=SIZE, antialias=True),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "resize_transformation = T.Compose([ T.Resize(SIZE) ])\n",
        "\n",
        "# Salva lo stato iniziale del modello\n",
        "initial_state = model.state_dict().copy()\n",
        "\n",
        "for index, img in enumerate(data_loader):\n",
        "    # Ripristina lo stato iniziale del modello\n",
        "    model = deepcopy(original_model)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "    # Azzera i gradienti prima di calcolare i nuovi\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Immagine ridimensionata (384x384)\n",
        "    img1 = resize_transformation(img[0])\n",
        "    # Immagini con augmentation\n",
        "    img2 = transforms(img[0])\n",
        "    img3 = transforms(img[0])\n",
        "    img4 = transforms(img[0])\n",
        "\n",
        "    # Concatena immagini\n",
        "    imgs = [img1, img2, img3, img4]\n",
        "    input = torch.cat(imgs, dim=0)\n",
        "\n",
        "    # Classificazione dell'immagine 1 prima delle augmentation\n",
        "    results = classify(model=model, img=img1)\n",
        "    results = elaborate_results(results=results)\n",
        "    predicted, results = results[\"predicted\"], results[\"results\"]\n",
        "\n",
        "    # Aggiorna accuracy della classificazione senza augmentation\n",
        "    if img[1].item() == predicted[\"index\"]:\n",
        "        accuracy_before = accuracy_before + 1\n",
        "\n",
        "    predicted_before = predicted[\"label\"]\n",
        "\n",
        "    # Calcola gli output delle immagini\n",
        "    output = model(input.to(DEVICE))\n",
        "\n",
        "    # Combina le probabilit√† delle immagini\n",
        "    probabilities = torch.nn.functional.softmax(output.logits, dim=-1).squeeze().to(DEVICE)\n",
        "\n",
        "    # Calcolo entropia\n",
        "    marginal = torch.mean(probabilities, dim=0).to(DEVICE)\n",
        "    entropy = -torch.sum(marginal * torch.log(marginal)).to(DEVICE)\n",
        "    entropy.backward()\n",
        "\n",
        "    # Gradient step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Classificazione dell'immagine 1 dopo le augmentation\n",
        "    results = classify(model=model, img=img1)\n",
        "    results = elaborate_results(results=results)\n",
        "    predicted, results = results[\"predicted\"], results[\"results\"]\n",
        "\n",
        "    #print(probabilities1 == probabilities2)\n",
        "    # Aggiorna accuracy della classificazione con augmentation\n",
        "    if img[1].item() == predicted[\"index\"]:\n",
        "        accuracy_after = accuracy_after + 1\n",
        "\n",
        "    label1 = f\"Image {index + 1} / {len(data_loader)}\"\n",
        "    label2 = f\"Accuracy before: {round((accuracy_before / (index + 1)) * 100, 1)}% ({accuracy_before} / {index + 1})\"\n",
        "    label3 = f\"Accuracy after: {round((accuracy_after / (index + 1)) * 100, 1)}% ({accuracy_after} / {index + 1})\"\n",
        "    label4 = f\"Diff: {round((accuracy_after / (index + 1)) * 100 - (accuracy_before / (index + 1)) * 100, 2)}%\"\n",
        "\n",
        "    print(f\"{label1} | {label2} | {label3} | {label4}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
