{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "GyLeV_sCPdGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaA4ccjTR4uV",
        "outputId": "42c184ee-ca36-41a1-a31e-89e0f8e015a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'dataset': No such file or directory\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=111HiEoEvZDdg1Y2EefI6n5dA_p4sMV4V\n",
            "From (redirected): https://drive.google.com/uc?id=111HiEoEvZDdg1Y2EefI6n5dA_p4sMV4V&confirm=t&uuid=f877bb88-3b9b-4179-b409-09b96355d80a\n",
            "To: /content/imagenet-a.tar\n",
            "100% 688M/688M [00:16<00:00, 40.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Init workspace\n",
        "!rm -r dataset\n",
        "!mkdir dataset\n",
        "\n",
        "# Download dataset and extract it\n",
        "!gdown 111HiEoEvZDdg1Y2EefI6n5dA_p4sMV4V\n",
        "!mv imagenet-a.tar ./dataset\n",
        "!tar -xf ./dataset/imagenet-a.tar\n",
        "!mv imagenet-a ./dataset\n",
        "\n",
        "# Cleanup\n",
        "!rm ./dataset/imagenet-a.tar\n",
        "\n",
        "# (optional) Upgrading pytorch for the latest augmentation functions\n",
        "#!pip install --upgrade torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUzDPvD_e8Fq",
        "outputId": "1768a0d1-1eca-4062-f3d4-5b2084bd6aa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1WKQGHjHUkIwZT0P2TpU9h-lY-6CnrsDd\n",
            "From (redirected): https://drive.google.com/uc?id=1WKQGHjHUkIwZT0P2TpU9h-lY-6CnrsDd&confirm=t&uuid=661980fb-e4dd-4dc3-b835-caa26d9b6733\n",
            "To: /content/imagenetv2-matched-frequency.tar.gz\n",
            "100% 1.26G/1.26G [00:22<00:00, 55.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Init workspace\n",
        "!rm -r dataset\n",
        "!mkdir dataset\n",
        "\n",
        "# Download dataset and extract it\n",
        "!gdown 1WKQGHjHUkIwZT0P2TpU9h-lY-6CnrsDd\n",
        "!mv imagenetv2-matched-frequency.tar.gz ./dataset\n",
        "!tar -xf ./dataset/imagenetv2-matched-frequency.tar.gz\n",
        "!mv imagenetv2-matched-frequency-format-val ./dataset\n",
        "\n",
        "# Cleanup\n",
        "!rm ./dataset/imagenetv2-matched-frequency.tar.gz\n",
        "\n",
        "# (optional) Upgrading pytorch for the latest augmentation functions\n",
        "#!pip install --upgrade torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ECdvhkFCpxci"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import basename, isfile, join\n",
        "from pathlib import Path\n",
        "import requests\n",
        "from contextlib import nullcontext\n",
        "from copy import deepcopy\n",
        "from typing import Union\n",
        "import random\n",
        "from io import BytesIO\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import zoom as scizoom\n",
        "import skimage as sk\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import v2\n",
        "from transformers import ViTForImageClassification\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xMgaZ3ON9hNd"
      },
      "outputs": [],
      "source": [
        "# Use cuda if available\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SIZE = (224, 224)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FlY1RBZTUQt7"
      },
      "outputs": [],
      "source": [
        "def show_image(img):\n",
        "    plt.imshow(img.squeeze(0).permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_batch_images(batch_tensor):\n",
        "    \"\"\"\n",
        "    Visualizza un batch di immagini contenute in un tensore di shape [B, C, H, W].\n",
        "\n",
        "    Parameters:\n",
        "    batch_tensor (torch.Tensor): Tensore contenente il batch di immagini.\n",
        "                                 Shape [B, C, H, W] dove B è il numero di immagini.\n",
        "    \"\"\"\n",
        "    batch_size = batch_tensor.shape[0]\n",
        "    fig, axs = plt.subplots(1, batch_size, figsize=(batch_size * 3, 3))\n",
        "\n",
        "    if batch_size == 1:\n",
        "        axs = [axs]  # Ensure axs is always a list\n",
        "\n",
        "    for i, ax in enumerate(axs):\n",
        "        # Converti il tensore in una immagine PIL e poi in un formato numpy per plt.imshow\n",
        "        img = transforms.ToPILImage()(batch_tensor[i])\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "XsQ4Eic_PRCG"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pOveYdeygYdb"
      },
      "outputs": [],
      "source": [
        "def load_imagenet_v2_labels() -> list[int]:\n",
        "\n",
        "    imagenet_v2 = \"./dataset/imagenetv2-matched-frequency-format-val\"\n",
        "\n",
        "    labels = [int(f) for f in listdir(imagenet_v2) if not isfile(join(imagenet_v2, f))]\n",
        "    labels.sort()\n",
        "\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "scjsACtS7Mzz"
      },
      "outputs": [],
      "source": [
        "def load_model_labels() -> list[str]:\n",
        "\n",
        "    url = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",
        "    path = Path(basename(url))\n",
        "\n",
        "    # Check if labels file already exists\n",
        "    if not path.exists():\n",
        "        response = requests.get(url)\n",
        "        path.write_text(response.text)\n",
        "\n",
        "    # Load labels\n",
        "    with open(path, \"r\") as f:\n",
        "        labels = json.load(f)\n",
        "\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "def load_resnet50(weights=ResNet50_Weights.DEFAULT):\n",
        "    return resnet50(weights=weights).to(DEVICE)"
      ],
      "metadata": {
        "id": "9DCYXcUfVuV8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mZ4Mo02l8cqQ"
      },
      "outputs": [],
      "source": [
        "def load_model(model_name: str = \"google/vit-base-patch16-224\") -> ViTForImageClassification:\n",
        "\n",
        "    # Load the pre-trained model\n",
        "    return ViTForImageClassification.from_pretrained(model_name).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X5w5Qab7nSxW"
      },
      "outputs": [],
      "source": [
        "class ImageNetV2(torch.utils.data.Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = load_imagenet_v2_labels() * 10\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        label = idx // 10\n",
        "\n",
        "        img_folder = os.path.join(self.img_dir, str(label))\n",
        "        img_path = [join(img_folder, f) for f in listdir(img_folder) if isfile(join(img_folder, f))][idx % (label if label != 0 else 1) - 1]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ut7hOuKqfnct"
      },
      "outputs": [],
      "source": [
        "def load_dataset(resize: bool = True) -> torch.utils.data.dataloader.DataLoader:\n",
        "\n",
        "    imagenet_v2 = \"./dataset/imagenetv2-matched-frequency-format-val\"\n",
        "\n",
        "    # Prepare data transformations for the train loader\n",
        "    transforms = [] if not resize else [T.Resize(SIZE)]\n",
        "    transforms.append(T.ToTensor())\n",
        "    transform = T.Compose(transforms)\n",
        "\n",
        "    # Load data\n",
        "    imagenet_v2_dataset = ImageNetV2(annotations_file=[], img_dir=imagenet_v2, transform=transform)\n",
        "    return torch.utils.data.DataLoader(imagenet_v2_dataset, 1, shuffle=True, num_workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Bk0WB8NHXXlQ"
      },
      "outputs": [],
      "source": [
        "def classify(model: ViTForImageClassification, img: torch.Tensor, no_grad: bool = True) -> dict:\n",
        "\n",
        "    # Use GPU if available\n",
        "    img = img.to(DEVICE)\n",
        "\n",
        "    # Perform inference\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad() if no_grad else nullcontext():\n",
        "        outputs = model(img)\n",
        "\n",
        "    # Extract probabilities from model's output logits\n",
        "    results = torch.nn.functional.softmax(outputs.logits, dim=-1).squeeze()\n",
        "\n",
        "    return results\n",
        "\n",
        "def classify_resnet50(model, img):\n",
        "\n",
        "    # Use GPU if available\n",
        "    img = img.to(DEVICE)\n",
        "\n",
        "    # Perform inference\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        probabilities = model(img)\n",
        "\n",
        "    return probabilities.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iLeoCaOmWG2u"
      },
      "outputs": [],
      "source": [
        "def elaborate_results(results: torch.Tensor) -> Union[dict, list]:\n",
        "\n",
        "    # Load model's labels\n",
        "    model_labels = load_model_labels()\n",
        "\n",
        "    if len(results.shape) == 1:\n",
        "        results = [results]\n",
        "\n",
        "    # Process results\n",
        "    final_results = []\n",
        "\n",
        "    for result in results:\n",
        "\n",
        "        item_results = {\n",
        "            \"predicted\": {},\n",
        "            \"results\": {}\n",
        "        }\n",
        "\n",
        "        predicted = None\n",
        "\n",
        "        for index, probability in enumerate(result):\n",
        "\n",
        "            item_results[\"results\"][index] = {\n",
        "                \"index\": index,\n",
        "                \"label\": model_labels[index],\n",
        "                \"probability\": probability.item()\n",
        "            }\n",
        "\n",
        "            if predicted is None or predicted[\"probability\"] < probability.item():\n",
        "                predicted = item_results[\"results\"][index]\n",
        "\n",
        "        item_results[\"predicted\"] = predicted\n",
        "\n",
        "        final_results.append(item_results)\n",
        "\n",
        "    return final_results if len(final_results) > 1 else final_results[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jrXGkChUYXHS"
      },
      "outputs": [],
      "source": [
        "# # Load model (only once)\n",
        "# model = load_model()\n",
        "\n",
        "# # Load data (only once)\n",
        "# data_loader = load_dataset()\n",
        "\n",
        "# # Evaluate the model (Accuracy: 18.37 %)\n",
        "# accuracy = 0\n",
        "\n",
        "# for index, img in enumerate(data_loader):\n",
        "\n",
        "#     # Get model prediction\n",
        "#     results = classify(model=model, img=img[0])\n",
        "#     results = elaborate_results(results=results)\n",
        "#     predicted, results = results[\"predicted\"], results[\"results\"]\n",
        "\n",
        "#     if img[1].item() == predicted[\"index\"]:\n",
        "#         accuracy = accuracy + 1\n",
        "\n",
        "#     print(f\"Image {index+1} / {len(data_loader)} | Accuracy: {round((accuracy / (index + 1)) * 100, 2)}% ({accuracy} / {index + 1})\")\n",
        "\n",
        "# accuracy = accuracy / len(data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4J0hHXdRfuOJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "AUGMENTATIONS\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"helper for zoom_blur\"\"\"\n",
        "def clipped_zoom(img, zoom_factor):\n",
        "    h = img.shape[0]\n",
        "    # ceil crop height(= crop width)\n",
        "    ch = int(np.ceil(h / zoom_factor))\n",
        "\n",
        "    top = (h - ch) // 2\n",
        "    img = scizoom(img[top:top + ch, top:top + ch], (zoom_factor, zoom_factor, 1), order=1)\n",
        "    # trim off any extra pixels\n",
        "    trim_top = (img.shape[0] - h) // 2\n",
        "\n",
        "    return img[trim_top:trim_top + h, trim_top:trim_top + h]\n",
        "\n",
        "\"\"\"helper for defocus_blur\"\"\"\n",
        "def disk(radius, alias_blur=0.1, dtype=np.float32):\n",
        "    if radius <= 8:\n",
        "        L = np.arange(-8, 8 + 1)\n",
        "        ksize = (3, 3)\n",
        "    else:\n",
        "        L = np.arange(-radius, radius + 1)\n",
        "        ksize = (5, 5)\n",
        "    X, Y = np.meshgrid(L, L)\n",
        "    aliased_disk = np.array((X ** 2 + Y ** 2) <= radius ** 2, dtype=dtype)\n",
        "    aliased_disk /= np.sum(aliased_disk)\n",
        "\n",
        "    # supersample disk to antialias\n",
        "    return cv2.GaussianBlur(aliased_disk, ksize=ksize, sigmaX=alias_blur)\n",
        "\n",
        "def saturate(x, severity=1):\n",
        "    c = [(0.3, 0), (0.1, 0), (2, 0), (5, 0.1), (20, 0.2)][severity - 1]\n",
        "\n",
        "    x = np.array(x) / 255.\n",
        "    x = sk.color.rgb2hsv(x)\n",
        "    x[:, :, 1] = np.clip(x[:, :, 1] * c[0] + c[1], 0, 1)\n",
        "    x = sk.color.hsv2rgb(x)\n",
        "\n",
        "    return np.clip(x, 0, 1) * 255\n",
        "\n",
        "def pixelate(x, severity=1):\n",
        "    c = [0.6, 0.5, 0.4, 0.3, 0.25][severity - 1]\n",
        "\n",
        "    x = x.resize((int(SIZE[0] * c), int(SIZE[0] * c)), Image.BOX)\n",
        "    x = x.resize(SIZE, Image.BOX)\n",
        "\n",
        "    return x\n",
        "\n",
        "def jpeg_compression(x, severity=1):\n",
        "    c = [25, 18, 15, 10, 7][severity - 1]\n",
        "\n",
        "    output = BytesIO()\n",
        "    x.save(output, 'JPEG', quality=c)\n",
        "    x = Image.open(output)\n",
        "\n",
        "    return x\n",
        "\n",
        "def brightness(x, severity=1):\n",
        "    c = [.1, .2, .3, .4, .5][severity - 1]\n",
        "\n",
        "    x = np.array(x) / 255.\n",
        "    x = sk.color.rgb2hsv(x)\n",
        "    x[:, :, 2] = np.clip(x[:, :, 2] + c, 0, 1)\n",
        "    x = sk.color.hsv2rgb(x)\n",
        "\n",
        "    return np.clip(x, 0, 1) * 255\n",
        "\n",
        "def zoom_blur(x, severity=1):\n",
        "    c = [np.arange(1, 1.11, 0.01),\n",
        "        np.arange(1, 1.16, 0.01),\n",
        "        np.arange(1, 1.21, 0.02),\n",
        "        np.arange(1, 1.26, 0.02),\n",
        "        np.arange(1, 1.31, 0.03)][severity - 1]\n",
        "\n",
        "    x = (np.array(x) / 255.).astype(np.float32)\n",
        "    out = np.zeros_like(x)\n",
        "    for zoom_factor in c:\n",
        "        zoomed = clipped_zoom(x, zoom_factor)\n",
        "        resized_zoomed = cv2.resize(zoomed, (out.shape[1], out.shape[0]))\n",
        "        out += resized_zoomed\n",
        "\n",
        "    x = (x + out) / (len(c) + 1)\n",
        "    return np.clip(x, 0, 1) * 255\n",
        "\n",
        "def defocus_blur(x, severity=1):\n",
        "    c = [(3, 0.1), (4, 0.5), (6, 0.5), (8, 0.5), (10, 0.5)][severity - 1]\n",
        "\n",
        "    x = np.array(x) / 255.\n",
        "    kernel = disk(radius=c[0], alias_blur=c[1])\n",
        "\n",
        "    channels = []\n",
        "    for d in range(3):\n",
        "        channels.append(cv2.filter2D(x[:, :, d], -1, kernel))\n",
        "    channels = np.array(channels).transpose((1, 2, 0))  # 3x224x224 -> 224x224x3\n",
        "\n",
        "    return np.clip(channels, 0, 1) * 255\n",
        "\n",
        "def gaussian_noise(x, severity=1):\n",
        "    c = [.08, .12, 0.18, 0.26, 0.38][severity - 1]\n",
        "\n",
        "    x = np.array(x) / 255.\n",
        "    return np.clip(x + np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n",
        "\n",
        "def shot_noise(x, severity=1):\n",
        "    c = [60, 25, 12, 5, 3][severity - 1]\n",
        "\n",
        "    x = np.array(x) / 255.\n",
        "    return np.clip(np.random.poisson(x * c) / c, 0, 1) * 255\n",
        "\n",
        "def impulse_noise(x, severity=1):\n",
        "    c = [.03, .06, .09, 0.17, 0.27][severity - 1]\n",
        "\n",
        "    x = sk.util.random_noise(np.array(x) / 255., mode='s&p', amount=c)\n",
        "    return np.clip(x, 0, 1) * 255\n",
        "\n",
        "def speckle_noise(x, severity=1):\n",
        "    c = [.15, .2, 0.35, 0.45, 0.6][severity - 1]\n",
        "\n",
        "    x = np.array(x) / 255.\n",
        "    return np.clip(x + x * np.random.normal(size=x.shape, scale=c), 0, 1) * 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NUkOrsxSf4mT"
      },
      "outputs": [],
      "source": [
        "def augment(img, augmentation_function, severity):\n",
        "  img = img * 255\n",
        "  img = img.to(torch.uint8)\n",
        "\n",
        "  img_np = img.numpy().transpose(0, 2, 3, 1)\n",
        "  augmented_np = augmentation_function(img_np, severity=severity)\n",
        "\n",
        "  augmented_tensor = torch.tensor(augmented_np, dtype=torch.uint8).permute(0, 3, 1, 2)\n",
        "  return augmented_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "-KFdzo1qjOxc"
      },
      "outputs": [],
      "source": [
        "# crea un batch di n immagini, in cui ognugna è una versione augmented dell'immagine\n",
        "# se n < numero augmentations allora ne prende una diversa ogni volta, altrimetni random\n",
        "\n",
        "def create_batch(img, n):\n",
        "  augmentations = [saturate, brightness, gaussian_noise, shot_noise, impulse_noise, speckle_noise]\n",
        "\n",
        "  a = set()\n",
        "  augmented = []\n",
        "  while len(augmented) < n:\n",
        "    i = random.randint(0, len(augmentations)-1)\n",
        "    if len(augmented) == len(augmentations):\n",
        "      a.clear()\n",
        "    if i in a: continue\n",
        "    a.add(i)\n",
        "    augmented_img = augment(img, augmentations[i], severity=2)\n",
        "    augmented.append(augmented_img)\n",
        "  return torch.cat(augmented, dim=0)\n",
        "\n",
        "\n",
        "from timm.data.auto_augment import rand_augment_transform\n",
        "from torchvision import transforms\n",
        "\n",
        "tfm = rand_augment_transform(\n",
        "    config_str=\"rand-m8-inc1-mstd1.01\",\n",
        "    hparams={}\n",
        ")\n",
        "\n",
        "# crea batch di n immagini, in cui ognuna è stata augmetned con timm\n",
        "def create_batch_timm(img, n=None, transform=tfm):\n",
        "    img = transforms.ToPILImage()(img.squeeze(0))\n",
        "    augmented = []\n",
        "    for _ in range(n):\n",
        "        augmented_img = transform(img)\n",
        "        augmented_tensor = transforms.ToTensor()(augmented_img).unsqueeze(0)\n",
        "        augmented.append(augmented_tensor)\n",
        "    return torch.cat(augmented, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643,
          "referenced_widgets": [
            "f1021f6e8c684d3f8b17ae4f84948b99",
            "b267d978eec34872891f3a657ac3b855",
            "8398379d3e894875bae1ffa76268f710",
            "51037a4f96fb450b88e8c4132e1eba3e",
            "08bcca8af01242f69185b4cfdfbd485d",
            "46ae44519fee446496721a37b2d6eb20",
            "80fb7b251842440aab3cfe7cfe7b392d",
            "1497b25b738a451f8f610c6ffc6f4519",
            "79067a65263d4703a3ba7dccdf6765fe",
            "970d6091700445d291ad8ac823ee2595",
            "28ff535a87b243bea8028297a713605a",
            "dfd96971ba0c4ff4a6ca2f7dc95bc02b",
            "ad06a6b3fe2945769322d18f007e357d",
            "e6089da52d1f41b7b882661f7ff9b6b4",
            "4722204d7f994846acaecfa025748ac3",
            "a92576ea79f342b9a29c2f10622879d3",
            "abd18fff9d324f97a3561512c0653d10",
            "f1a62ef2ec724024a13e618f19dd9fde",
            "338e15ccac60479aa2a64a0cf8f90d3b",
            "c8e11af396ad481eae79052fbfb525c8",
            "d1484eae70864e0aad8696215b8660a5",
            "ff9f09b123b440958cc1be66a7a1bbc1"
          ]
        },
        "id": "IzaEQC-_dzji",
        "outputId": "742f47c6-2227-49dd-9470-c6f9dcbca980"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1021f6e8c684d3f8b17ae4f84948b99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfd96971ba0c4ff4a6ca2f7dc95bc02b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 1 / 10000 | Accuracy before: 100.0% (1 / 1) | Accuracy after: 100.0% (1 / 1) | Diff: 0.0%\n",
            "Image 2 / 10000 | Accuracy before: 100.0% (2 / 2) | Accuracy after: 100.0% (2 / 2) | Diff: 0.0%\n",
            "Image 3 / 10000 | Accuracy before: 100.0% (3 / 3) | Accuracy after: 100.0% (3 / 3) | Diff: 0.0%\n",
            "Image 4 / 10000 | Accuracy before: 75.0% (3 / 4) | Accuracy after: 75.0% (3 / 4) | Diff: 0.0%\n",
            "Image 5 / 10000 | Accuracy before: 80.0% (4 / 5) | Accuracy after: 80.0% (4 / 5) | Diff: 0.0%\n",
            "Image 6 / 10000 | Accuracy before: 83.3% (5 / 6) | Accuracy after: 83.3% (5 / 6) | Diff: 0.0%\n",
            "Image 7 / 10000 | Accuracy before: 85.7% (6 / 7) | Accuracy after: 85.7% (6 / 7) | Diff: 0.0%\n",
            "Image 8 / 10000 | Accuracy before: 87.5% (7 / 8) | Accuracy after: 87.5% (7 / 8) | Diff: 0.0%\n",
            "Image 9 / 10000 | Accuracy before: 88.9% (8 / 9) | Accuracy after: 88.9% (8 / 9) | Diff: 0.0%\n",
            "Image 10 / 10000 | Accuracy before: 90.0% (9 / 10) | Accuracy after: 80.0% (8 / 10) | Diff: -10.0%\n",
            "Image 11 / 10000 | Accuracy before: 90.9% (10 / 11) | Accuracy after: 81.8% (9 / 11) | Diff: -9.09%\n",
            "Image 12 / 10000 | Accuracy before: 91.7% (11 / 12) | Accuracy after: 83.3% (10 / 12) | Diff: -8.33%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-b7af9159581f>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Immagine ridimensionata (384x384)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mimg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize_transformation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-a9a92c6540dc>\u001b[0m in \u001b[0;36mcreate_batch\u001b[0;34m(img, n)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0maugmented_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmentations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseverity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0maugmented\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-d15ce8ceaddc>\u001b[0m in \u001b[0;36maugment\u001b[0;34m(img, augmentation_function, severity)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mimg_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0maugmented_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmentation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseverity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseverity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0maugmented_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-c626010e6039>\u001b[0m in \u001b[0;36mbrightness\u001b[0;34m(x, severity)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb2hsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhsv2rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py\u001b[0m in \u001b[0;36mfixed_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchannel_axis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;31m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skimage/color/colorconv.py\u001b[0m in \u001b[0;36mrgb2hsv\u001b[0;34m(rgb, channel_axis)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mout_h\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdelta\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseterr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mold_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;31m# -- output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/_ufunc_config.py\u001b[0m in \u001b[0;36mseterr\u001b[0;34m(all, divide, over, under, invalid)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mset_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mseterr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdivide\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \"\"\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Supponendo che load_dataset, classify_image, merged_labels e il modello siano definiti altrove\n",
        "accuracy_before = 0\n",
        "accuracy_after = 0\n",
        "\n",
        "# Load model (only once)\n",
        "model = load_model()\n",
        "original_model = deepcopy(model)\n",
        "\n",
        "data_loader = load_dataset(resize=False)\n",
        "\n",
        "# transformation = T.Compose([\n",
        "#         T.Resize((500, 500)),\n",
        "#         T.CenterCrop((384, 384)) ])\n",
        "\n",
        "transforms = v2.Compose([\n",
        "    v2.RandomResizedCrop(size=SIZE, antialias=True),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "resize_transformation = T.Compose([ T.Resize(SIZE) ])\n",
        "\n",
        "# Salva lo stato iniziale del modello\n",
        "initial_state = model.state_dict().copy()\n",
        "\n",
        "for index, img in enumerate(data_loader):\n",
        "    # Ripristina lo stato iniziale del modello\n",
        "    model = deepcopy(original_model)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "    # Azzera i gradienti prima di calcolare i nuovi\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Immagine ridimensionata (384x384)\n",
        "    img1 = resize_transformation(img[0])\n",
        "    batch = create_batch(img[0], 5)\n",
        "    inputs = transforms(batch)\n",
        "\n",
        "    # Classificazione dell'immagine 1 prima delle augmentation\n",
        "    results = classify(model=model, img=img1)\n",
        "    results = elaborate_results(results=results)\n",
        "    predicted, results = results[\"predicted\"], results[\"results\"]\n",
        "\n",
        "    # Aggiorna accuracy della classificazione senza augmentation\n",
        "    if img[1].item() == predicted[\"index\"]:\n",
        "        accuracy_before = accuracy_before + 1\n",
        "\n",
        "    predicted_before = predicted[\"label\"]\n",
        "\n",
        "    # Calcola gli output delle immagini\n",
        "    output = model(inputs.to(DEVICE))\n",
        "\n",
        "    # Combina le probabilità delle immagini\n",
        "    probabilities = torch.nn.functional.softmax(output.logits, dim=-1).squeeze().to(DEVICE)\n",
        "\n",
        "    # Calcolo entropia\n",
        "    marginal = torch.mean(probabilities, dim=0).to(DEVICE)\n",
        "    entropy = -torch.sum(marginal * torch.log(marginal)).to(DEVICE)\n",
        "    entropy.backward()\n",
        "\n",
        "    # Gradient step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Classificazione dell'immagine 1 dopo le augmentation\n",
        "    results = classify(model=model, img=img1)\n",
        "    results = elaborate_results(results=results)\n",
        "    predicted, results = results[\"predicted\"], results[\"results\"]\n",
        "\n",
        "    #print(probabilities1 == probabilities2)\n",
        "    # Aggiorna accuracy della classificazione con augmentation\n",
        "    if img[1].item() == predicted[\"index\"]:\n",
        "        accuracy_after = accuracy_after + 1\n",
        "\n",
        "    label1 = f\"Image {index + 1} / {len(data_loader)}\"\n",
        "    label2 = f\"Accuracy before: {round((accuracy_before / (index + 1)) * 100, 1)}% ({accuracy_before} / {index + 1})\"\n",
        "    label3 = f\"Accuracy after: {round((accuracy_after / (index + 1)) * 100, 1)}% ({accuracy_after} / {index + 1})\"\n",
        "    label4 = f\"Diff: {round((accuracy_after / (index + 1)) * 100 - (accuracy_before / (index + 1)) * 100, 2)}%\"\n",
        "\n",
        "    print(f\"{label1} | {label2} | {label3} | {label4}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f1021f6e8c684d3f8b17ae4f84948b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b267d978eec34872891f3a657ac3b855",
              "IPY_MODEL_8398379d3e894875bae1ffa76268f710",
              "IPY_MODEL_51037a4f96fb450b88e8c4132e1eba3e"
            ],
            "layout": "IPY_MODEL_08bcca8af01242f69185b4cfdfbd485d"
          }
        },
        "b267d978eec34872891f3a657ac3b855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46ae44519fee446496721a37b2d6eb20",
            "placeholder": "​",
            "style": "IPY_MODEL_80fb7b251842440aab3cfe7cfe7b392d",
            "value": "config.json: 100%"
          }
        },
        "8398379d3e894875bae1ffa76268f710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1497b25b738a451f8f610c6ffc6f4519",
            "max": 69665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79067a65263d4703a3ba7dccdf6765fe",
            "value": 69665
          }
        },
        "51037a4f96fb450b88e8c4132e1eba3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_970d6091700445d291ad8ac823ee2595",
            "placeholder": "​",
            "style": "IPY_MODEL_28ff535a87b243bea8028297a713605a",
            "value": " 69.7k/69.7k [00:00&lt;00:00, 3.21MB/s]"
          }
        },
        "08bcca8af01242f69185b4cfdfbd485d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46ae44519fee446496721a37b2d6eb20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80fb7b251842440aab3cfe7cfe7b392d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1497b25b738a451f8f610c6ffc6f4519": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79067a65263d4703a3ba7dccdf6765fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "970d6091700445d291ad8ac823ee2595": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28ff535a87b243bea8028297a713605a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfd96971ba0c4ff4a6ca2f7dc95bc02b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad06a6b3fe2945769322d18f007e357d",
              "IPY_MODEL_e6089da52d1f41b7b882661f7ff9b6b4",
              "IPY_MODEL_4722204d7f994846acaecfa025748ac3"
            ],
            "layout": "IPY_MODEL_a92576ea79f342b9a29c2f10622879d3"
          }
        },
        "ad06a6b3fe2945769322d18f007e357d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abd18fff9d324f97a3561512c0653d10",
            "placeholder": "​",
            "style": "IPY_MODEL_f1a62ef2ec724024a13e618f19dd9fde",
            "value": "model.safetensors: 100%"
          }
        },
        "e6089da52d1f41b7b882661f7ff9b6b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_338e15ccac60479aa2a64a0cf8f90d3b",
            "max": 346293852,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8e11af396ad481eae79052fbfb525c8",
            "value": 346293852
          }
        },
        "4722204d7f994846acaecfa025748ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1484eae70864e0aad8696215b8660a5",
            "placeholder": "​",
            "style": "IPY_MODEL_ff9f09b123b440958cc1be66a7a1bbc1",
            "value": " 346M/346M [00:02&lt;00:00, 169MB/s]"
          }
        },
        "a92576ea79f342b9a29c2f10622879d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abd18fff9d324f97a3561512c0653d10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1a62ef2ec724024a13e618f19dd9fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "338e15ccac60479aa2a64a0cf8f90d3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8e11af396ad481eae79052fbfb525c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1484eae70864e0aad8696215b8660a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff9f09b123b440958cc1be66a7a1bbc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}